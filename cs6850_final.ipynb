{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import networkx as nx\n",
    "from scipy.stats import kendalltau\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40001/40001 [01:12<00:00, 552.31it/s]  \n"
     ]
    }
   ],
   "source": [
    "txts = glob.glob(\"C:/Users/yiwei/Downloads/wiki_data/*.txt\")\n",
    "titles = []\n",
    "texts = []\n",
    "for val in tqdm(txts):\n",
    "    with open(val) as f:\n",
    "        text = f.read()\n",
    "        title = text[text.find('>') + 1:text.find('<', 2)]\n",
    "        texts.append(text)\n",
    "        titles.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame({'title': titles, 'text': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>April</td>\n",
       "      <td>&lt;title&gt;April&lt;/title&gt;&lt;text&gt;{{monththisyear|4}} ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>August</td>\n",
       "      <td>&lt;title&gt;August&lt;/title&gt;&lt;text&gt;{{monththisyear|8}}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andouille</td>\n",
       "      <td>&lt;title&gt;Andouille&lt;/title&gt;&lt;text&gt;[[File:Andouille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calculus</td>\n",
       "      <td>&lt;title&gt;Calculus&lt;/title&gt;&lt;text&gt;{{More citations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Liter</td>\n",
       "      <td>&lt;title&gt;Liter&lt;/title&gt;&lt;text&gt;#REDIRECT [[Litre]] ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title                                               text\n",
       "0      April  <title>April</title><text>{{monththisyear|4}} ...\n",
       "1     August  <title>August</title><text>{{monththisyear|8}}...\n",
       "2  Andouille  <title>Andouille</title><text>[[File:Andouille...\n",
       "3   Calculus  <title>Calculus</title><text>{{More citations ...\n",
       "4      Liter  <title>Liter</title><text>#REDIRECT [[Litre]] ..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "for val in range(len(corpus['title'])):\n",
    "    try:\n",
    "        if 'template' in corpus['title'][val].lower() or 'category' in corpus['title'][val].lower():\n",
    "            to_drop.append(val)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv('wiki_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('wiki_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because dataset is too big to handle with our resources (limitations for semantic graph), we subsample the data\n",
    "subset_corpus = corpus.sample(frac=0.25, random_state=42)\n",
    "subset_corpus.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus.to_csv('subset_wiki_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus = pd.read_csv('subset_wiki_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Semantic Graph from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:00<00:00, 9549.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# removing punctuation and stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "to_tokenize = []\n",
    "for article in tqdm(subset_corpus['text']):\n",
    "    cleaned_text = cleanhtml(article)\n",
    "    tokens = tokenizer.tokenize(cleaned_text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    to_tokenize.append(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_title_mapping = {idx: str(subset_corpus['title'][idx]).lower() for idx in range(len(to_tokenize))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:03<00:00, 2346.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_text_embedding(tokens, model):\n",
    "    word_vectors = []\n",
    "    for word in tokens:\n",
    "        if word in model.key_to_index: \n",
    "            word_vectors.append(model[word])\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "text_embeddings = np.array([get_text_embedding(tokens, model) for tokens in tqdm(to_tokenize)])\n",
    "\n",
    "cos_sim_matrix = cosine_similarity(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:40<00:00, 193.97it/s] \n"
     ]
    }
   ],
   "source": [
    "SG = nx.Graph()\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "for i in tqdm(range(len(to_tokenize))):\n",
    "    SG.add_node(i)\n",
    "    for j in range(i+1, len(to_tokenize)):\n",
    "        similarity = cos_sim_matrix[i, j]\n",
    "        if abs(similarity) >= threshold:\n",
    "            SG.add_edge(i, j, weight=similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating PageRank Graph from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [02:05<00:00, 63.01it/s] \n"
     ]
    }
   ],
   "source": [
    "PRG = nx.DiGraph()\n",
    "pattern = r'\\[\\[(.*?)\\]\\]'\n",
    "titles = subset_corpus['title'].str.lower()\n",
    "PRG.add_nodes_from(titles)\n",
    "for i in tqdm(range(len(subset_corpus['title']))):\n",
    "    txt = subset_corpus['text'][i]\n",
    "    references = re.findall(pattern, txt)\n",
    "    for reference in references:\n",
    "        if len(titles[titles == reference]) > 0:\n",
    "            PRG.add_edge(titles[i], reference.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Semantic Graph for PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_semantic = nx.pagerank(SG, weight='weight')\n",
    "pr_semantic = {idx_title_mapping[idx]: pr_semantic[idx] for idx in pr_semantic}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Normal Dataset for PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(PRG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Semantic Graph vs Normal Dataset for PageRank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(input):\n",
    "    keys = list(input.keys())\n",
    "    values = list(input.values())\n",
    "    sorted_value_idx = np.argsort(values)\n",
    "    return [keys[i] for i in sorted_value_idx[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_sorted = sort_dict(pr)\n",
    "pr_semantic_sorted = sort_dict(pr_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.008867568973233186, pvalue=0.23795417239008265)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendalltau(pr_sorted, pr_semantic_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aleister crowley',\n",
       " 'history of graphic design',\n",
       " 'romania',\n",
       " 'chuck berry',\n",
       " 'gregor mendel',\n",
       " 'port arthur, tasmania',\n",
       " 'bristol',\n",
       " '1985',\n",
       " 'human sacrifice',\n",
       " 'joseph beuys']"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_semantic_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['government',\n",
       " 'language',\n",
       " 'species',\n",
       " 'actor',\n",
       " 'money',\n",
       " 'circle',\n",
       " 'currency',\n",
       " '2007',\n",
       " 'air',\n",
       " 'computer']"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unweighted Semantic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7908 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:07<00:00, 1046.52it/s]\n"
     ]
    }
   ],
   "source": [
    "SGU = nx.DiGraph()\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "for i in tqdm(range(len(to_tokenize))):\n",
    "    SGU.add_node(i)\n",
    "    for j in range(i+1, len(to_tokenize)):\n",
    "        similarity = cos_sim_matrix[i, j]\n",
    "        if similarity >= threshold:\n",
    "            SGU.add_edge(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_semantic_unweighted = nx.pagerank(SGU, weight='weight')\n",
    "pr_semantic_unweighted = {idx_title_mapping[idx]: pr_semantic_unweighted[idx] for idx in pr_semantic_unweighted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_semantic_unweighted_sorted = sort_dict(pr_semantic_unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=-0.00029786599255738196, pvalue=0.968379604691019)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendalltau(pr_sorted, pr_semantic_unweighted_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.001246908652547464, pvalue=0.8682030881727664)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendalltau(pr_semantic_sorted, pr_semantic_unweighted_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the new york times',\n",
       " '2007',\n",
       " 'isle of man',\n",
       " 'joseph beuys',\n",
       " 'bad zurzach',\n",
       " 'silkworm',\n",
       " 'binomial nomenclature',\n",
       " 'hedgehog',\n",
       " 'janice raymond',\n",
       " 'new brunswick']"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_semantic_unweighted_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating graph function\n",
    "\n",
    "def create_graph(weighted = True, alpha = 0.1, user_input = None, cos_sim_matrix = cos_sim_matrix):\n",
    "    # alpha is the semantic value score, 1 - alpha is link score\n",
    "    OUT_GRAPH = nx.Graph()\n",
    "    matrix_len = cos_sim_matrix.shape[0]\n",
    "    if user_input:\n",
    "        tokenized_user_input = tokenizer.tokenize(cleanhtml(user_input))\n",
    "        filtered_user_input = [word for word in tokenized_user_input if word not in stop_words]\n",
    "        user_embedding = get_text_embedding(filtered_user_input, model)\n",
    "        cos_sim_matrix = cosine_similarity(np.vstack([text_embeddings, user_embedding]))\n",
    "        matrix_len = cos_sim_matrix.shape[0]\n",
    "        idx_title_mapping[matrix_len - 1] = f'user input: {user_input}'\n",
    "    for i in tqdm(range(matrix_len)):\n",
    "        OUT_GRAPH.add_node(i)\n",
    "        for j in range(i+1, matrix_len):\n",
    "            similarity = cos_sim_matrix[i, j] \n",
    "            link = int((idx_title_mapping[i], idx_title_mapping[j]) in PRG.edges) if j < matrix_len - 1 and i < matrix_len - 1 else 0\n",
    "            if weighted:\n",
    "                score = similarity * alpha + (1 - alpha) * link if abs(similarity) >= 0.1 else (1 - alpha) * link\n",
    "            else:\n",
    "                if user_input:\n",
    "                    score = alpha + (1 - alpha) * link if similarity >= 0.1 else 0\n",
    "                else:\n",
    "                    score = alpha + (1 - alpha) * link if similarity >= 0.8 else (1 - alpha) * link\n",
    "            if score:\n",
    "                OUT_GRAPH.add_edge(i, j, weight=score)\n",
    "    return OUT_GRAPH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.075, 0.1, 0.125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_top_ten(OUT_GRAPH, user_input = None):\n",
    "    evaluated = nx.pagerank(OUT_GRAPH, weight='weight')\n",
    "    evaluated = {idx_title_mapping[idx]: evaluated[idx] for idx in evaluated}\n",
    "    evaluated_sorted = sort_dict(evaluated)\n",
    "    return evaluated_sorted[:10], evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Graph (combine using linear combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting alpha value 0.075...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:51<00:00, 154.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting alpha value 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:52<00:00, 150.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting alpha value 0.125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:53<00:00, 147.51it/s] \n"
     ]
    }
   ],
   "source": [
    "weighted_t10 = {}\n",
    "weighted_rankings = {}\n",
    "for alpha in alphas:\n",
    "    print(f'Starting alpha value {alpha}...')\n",
    "    OUT_GRAPH = create_graph(True, alpha)\n",
    "    top_ten, ranking_vals = evaluate_top_ten(OUT_GRAPH)\n",
    "    weighted_t10[alpha] = top_ten\n",
    "    weighted_rankings[alpha] = ranking_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha value 0.075:\n",
      "Top ten articles are 2007, 2002, wikipedia:list of common misspellings, 2001, 1990, 1988, 1986, 1949, 1927, 1916\n",
      "For alpha value 0.1:\n",
      "Top ten articles are 2007, 2002, 2001, wikipedia:list of common misspellings, 1990, 1988, 1986, 1949, 1993, 1927\n",
      "For alpha value 0.125:\n",
      "Top ten articles are 2007, 2002, 2001, 1990, wikipedia:list of common misspellings, 1988, 1993, 1986, 1949, 1916\n"
     ]
    }
   ],
   "source": [
    "for alpha in weighted_t10:\n",
    "    print(f'For alpha value {alpha}:')\n",
    "    print(f'Top ten articles are {\", \".join(weighted_t10[alpha])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unweighted Graph (combine using linear combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting alpha value 0.075...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:17<00:00, 455.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting alpha value 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:14<00:00, 528.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting alpha value 0.125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7908/7908 [00:14<00:00, 536.99it/s] \n"
     ]
    }
   ],
   "source": [
    "unweighted_t10 = {}\n",
    "unweighted_rankings = {}\n",
    "for alpha in alphas:\n",
    "    print(f'Starting alpha value {alpha}...')\n",
    "    OUT_GRAPH = create_graph(False, alpha)\n",
    "    top_ten, ranking_vals = evaluate_top_ten(OUT_GRAPH)\n",
    "    unweighted_t10[alpha] = top_ten\n",
    "    unweighted_rankings[alpha] = ranking_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha value 0.075:\n",
      "Top ten articles are wikipedia:list of common misspellings, wikipedia:basic english ordered wordlist, 2007, list of years, 2002, language, 1988, 2001, 1990, 1986\n",
      "For alpha value 0.1:\n",
      "Top ten articles are wikipedia:list of common misspellings, wikipedia:basic english ordered wordlist, 2007, list of years, 2002, 1988, 2001, language, 1990, 1986\n",
      "For alpha value 0.125:\n",
      "Top ten articles are wikipedia:list of common misspellings, wikipedia:basic english ordered wordlist, 2007, 2002, list of years, 1988, 2001, 1990, language, curitiba\n"
     ]
    }
   ],
   "source": [
    "for alpha in unweighted_t10:\n",
    "    print(f'For alpha value {alpha}:')\n",
    "    print(f'Top ten articles are {\", \".join(unweighted_t10[alpha])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Combined Graph for User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_results(graph):\n",
    "    scores = [val for val in sorted(graph.edges(data=True),key= lambda x: x[2]['weight'],reverse=True) if len(to_tokenize) in val]\n",
    "    search_results = [idx_title_mapping[val[0]] for val in scores]\n",
    "    return search_results, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Graph Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7909/7909 [00:51<00:00, 154.68it/s] \n"
     ]
    }
   ],
   "source": [
    "USER_WEIGHTED_GRAPH = create_graph(True, 0.125, 'funny movies')\n",
    "user_search_results, user_scores = sorted_results(USER_WEIGHTED_GRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parody',\n",
       " '2007 in film',\n",
       " 'trailer (movie)',\n",
       " 'christopher walken',\n",
       " 'high school musical',\n",
       " 'philadelphia (movie)',\n",
       " 'sunrise (movie)',\n",
       " 'tron',\n",
       " 'rounders (movie)',\n",
       " 'office space']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_search_results[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Graph Unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7909/7909 [00:46<00:00, 170.37it/s] \n"
     ]
    }
   ],
   "source": [
    "USER_UNWEIGHTED_GRAPH = create_graph(False, 0.125, 'funny movies')\n",
    "user_search_results, user_scores = sorted_results(USER_UNWEIGHTED_GRAPH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight of the british empire',\n",
       " 'miss world',\n",
       " 'world war',\n",
       " 'tripoli, greece',\n",
       " 'jungle',\n",
       " 'pathogen',\n",
       " 'message (computer science)',\n",
       " 'adam elliot',\n",
       " 'bush',\n",
       " 'edinburgh castle']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_search_results[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3350",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
